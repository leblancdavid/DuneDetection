\section{Background}
\subsection{Dune Pattern Studies}

In \cite{Kocurek_Ewing}, an explanation of how dune-field patterns emerge and addresses the degree of complexity from the standpoint of self-organizing system. Dune patterns are classified into two basic categories: simple or complex. According to this paper, a simple pattern is defined as having a single pattern type. Complex patterns are said to potentially have multiple spatially superimposed pattern types. Simple patterns trend towards a better ordering, so long as the wind regime remains constant. Changes in the trend of wind pattern will cause reorientation. Complex patterns can then be interpreted as the superposition of many generations of wind regimes.

Complexity can be measured using pattern analysis, measuring parameters such as the crest length, orientation, spacing,	defect density, and other properties. These properties would be very useful information to extract for the dune detection. Finding crest-lines	can help us identify the patterns, extracting meaningful data such as junctions, terminations, mergers, linking, and other interesting processes explained in \cite{Kocurek_Ewing}.

In later work, \cite{Ewing_Peyret_Kocurek_Bourke} studied the dune field pattern formations on the north polar region of Mars. These dunes are thought to be mostly inactive, with relative	no movement over a period of 4 to15 Martian years. The reason this region is interesting is because there are two nearly orthogonal crest-line orientations present in the region. This complex system of superposition of multiple patterns in this example showcase a set of \emph{primary} and \emph{secondary} crest-lines which would be valuable data to automatically segment. The \emph{primary}	dunes are the largest-scale dunes, extend over the entire length of the image set, and contain many \emph{Y} junctions,	an indication of well-organized linear dunes. In contrast, \emph{secondary} dunes are rounded, have least defined features, and are perpendicular	to the main primary crest-lines.

Other interesting features are the so-called \emph{Slipfaces}. These typically appear along the \emph{primary} crest-lines, in areas of intersection with the \emph{primary} and \emph{secondary} crest-lines. Another feature are the \emph{Wind Ripples}. The ripples are present on the surface of most dunes with the exception	of \emph{slipfaces}. To detect	these types of features, a higher resolution image is a required, as those types of features are very small compared to the scale of the \emph{primary} and \emph{secondary}	dunes. \emph{Interdune Areas} are features specific to the area studied in \cite{Ewing_Peyret_Kocurek_Bourke},	have a polygonal shape, and are indicative of ice.

The rest of the paper describes an in depth statistical analysis of the features to compute the flow fields and understand the geomorphic relationships which are present in the area. The paper \cite{Ewing_Peyret_Kocurek_Bourke} provides	a lot of interesting and valuable information which can be used to understand the application of automatic dune detection. If these features can be extracted, it would be interesting to see if we can do a similar statistical analysis based on the results presented. The next step would be to retrieve the HiRISE dataset and their data or ground truth to compare.

In the most recent work, \cite{Multi_spatial_analysis_aeolian_dune_field_patterns} study the aeolian dune-fields at different scales. Scale is an important factor to consider because aeolian dune-fields patterns can vary over	a wide range of scales, both spatially and temporally. Being able to measure the change of scale over time is important in order to investigate the environmental conditions of the studied region. Aeolian	dunes are developed typically in transitions from sand patches, to proto-dunes, to dunes, to dune-field patterns. Complex dune patterns are usually a juxtaposition of simple dune patterns are multiple scales. To summarize, being able to detect the crest-lines and other types of features at multiple scales is invaluable.

In other related work, \cite{Application_spatial_cross_correlation_detection_submarine_dunes} studied the migration of submarine sand dunes. The dataset used in this paper includes digital terrain models (DTMs) retrieved from high-density multibeam echosounders (MBES) taken of submarine sand dunes along the coast of New Brunswick. In order to measure the migration of these types of dunes, the motion was measured by simply subtracting the 	DTMs from sequences over time. The implementation uses a simple cross-correlation to find similarities from one set to the next. From the correlation matches found, the migration vector can be computed. The migration data processing can extract the flow fields of the dunes.

\subsection{Image Processing}

\subsubsection{Filtering}

There are many common filtering approaches used	in image processing and computer vision applications. In this application, satellite images of sand dunes may include various amounts of noise. In order to improve the dune detection algorithms, the goal of filtering is to filter out the noise while preserving or enhancing the necessary features such as the crest-line edges.

In \cite{Bilateral-filtering-gray-color-images}, the bilateral filter is proposed, which allows flat noisy regions	to be filtered while preserving strong edges. Bilateral filtering	is a simple non-iterative algorithm for filtering out noise.


\subsubsection{Illumination Normalization}

Illumination normalization is a process in which the image illumination is made to be more evenly distributed across the image, to make the illumination invariant in conditions of varying lighting. This processing is most commonly used in face detection or recognition applications. In \cite{Illumination_normalization_based_on_2d_gaussian_model}, a Gaussian illumination model is used to stretch the contrasts of darker areas. The process involves using a Quadtree method to divide the image into sub regions to find dark areas. The areas that meet these requirements are processed with the Gaussian illumination model to brighten them up, which evens out the illumination. Applying this normalization significantly improves performance of face recognition.

In a similar approach \cite{Illumination_normalization_for_image_restoration_using_modified_retinex_algorithm}, the authors use a modified retinex algorithm to restore poorly illuminated areas of an image. According to Lambertian reflectance theory, images consist of two components, reflectance and illumination. If an image contains a set of small scale features, and large scale features, and illumination is only applied to the large-scale set, then the small scale features can be preserved. In this approach, the normalization is applied to small-scale and large-scale features independently. The Single Scale Retinex process is applied to the image to separate	the reflectance (small scale) and illumination (large scale). After, images are thresholded and histogram equalization is applied, which spreads out intensity values of darker areas of the image.

In a remote sensing application, \cite{Illumination_normalization_among_multiple_remote_senging_images},	illumination normalization in the gradient domain and using singular	value equalization. According to the authors, the gradient domain image enhancement process can compress the dynamic range of images, increase contrast, and enhance fine details in darker regions. The goal of singular value equalization is to make all images in a specific set to have similar mean intensity value. Some images may have a lower or higher mean intensity. By using SVE, the pixels can be processed to achieve a mean closer to the target intensity, therefore normalizing illumination accross a set of images. The drawback of this method is that multiple images of the same or similar scenes are required.

\subsubsection{Edge and Line Detection}

The Canny edge detector is a well known and used method to retrieve important edge features from an image. In \cite{Canny_edge_detection_enhancement_scale_multiplication}, 	enhancement to the method is proposed by analyzing the responses of detection at two scales. The benefits of this technique are better localization in images with larger amounts of noise, at the cost of	a slightly lower detection rate. Overall the performance of the Canny operator is improved by using multiple scales.

The Canny detector is further improved in \cite{Runway_detection_tracking_unmanned} for a runway detection application for unmanned aerial vehicles. The main contribution of this paper is the use the Canny operator combined with a mean filter, and using the Hough Transform to track runways. The advantages o f the mean filtering is that it filters out noise, preserves edges (better than a traditional Gaussian filter), and makes the image less fuzzy. To solve the problem of the dual threshold of the Canny operator, the proposed method is to compute the thresholds dynamically based on averages.

In \cite{Improved_Canny_Edge_Detection}, an improved implementation of the popular Canny edge detector is proposed. According to the authors, the traditional Canny algorithm suffers from two main	problems: the gradient calculation is sensitive to noise and the use of the fixed double threshold may not be suitable for images with high gradient variability. The main flaw in the gradient calculation has to do with the inequality of the edge detection in darker versus brighter regions. For the parameter selection of the double threshold, the paper propose choosing the high and low thresholds from the computed mean and standard deviation of the gradient magnitudes.

A line detection and image enhancement has been proposed in \cite{Robust_Faint_Line_Detection_Enhancement_Algorithm}, for a digital image restoration of heritage art application. The images worked with have been deteriorated over time, and contain many cracks, faint lines, and broken stroke. The goal of this is to remove unwanted edges, and enhance the desired edges. The approach is implemented in three basic steps: Initial line detection with non-maxima suppression, true line detection using anisotropic refinement, and noise reduction.

The initial step is to perform correlation convolving the image with some sort of edge detecting mask, rotated at different orientations, and retrieving the maximum response for the orientation	for each pixel. The paper claims this produces a sharper and more reliable edge map, which is then filtered using non-maxima suppression. This process preserves strong lines while removing texture lines. The image is the binarized to remove all remaining weak edges, and only allowing strong edges to remain. The smoothing is applied to the original based on the processing done, smoothing out weak edges, while not smoothing strong true edges.

In a similar approach, \cite{Image_Edge_detection_Rotating_Kernel_Transformation} use rotating edge detection kernels and fuse the responses from the operations. What determines what an edge is a location and a direction, as a non-edge point such as a flat area or noise, which has no specific direction. Following this principle, the response in on direction should be higher than in the other directions. For a non-edge, the	responses in each of the directions are similar. Therefore computing the mean and standard deviation of each of the kernel responses can help resolve edge versus non edge pixels. The results obtained from this process shows promising results when applied to noisy images. The approach proposed in \cite{Image_Edge_detection_Rotating_Kernel_Transformation} has definite potential for improved edge detection applied to dune crest-line detection. It also has potential for improvement of the method itself.

An interesting alternative to the classical Hough transform is presented in \cite{Automated_cable_tracking_sonar_imagery} in an application inspection or maintenance of underwater cable components. The paper discusses image processing techniques to extract linear features in cluttered and noisy images. After applying some anisotropic filtering to remote high frequency noise, edges and lines are detected on the sonar based images. To detect potential linear features, they use a Phase Congruency detector. The Hough transform is then used to find linear features, and a criteria is determined to reject false positives and preserve true positives.

\subsubsection{Edge Linking}

In \cite{Edge_linking_using_geodesic_distance_neighborhood_information}, an edge linking approach is proposed which using local neighborhood, using geodesic distance (as opposed to the common Euclidean distance measure) between edge candidates. When calculating the direction of edge end points, typically eight directions are used, which introduces error. To address this \cite{Edge_linking_using_geodesic_distance_neighborhood_information} define a windows size based on the maximum allowable edge gap, and fit a line to the edges, which allows a full range of directions to be accounted for. The geodesic distance measurement is not only based on euclidean distance but also on intensity values of the image, which reportedly gives better results.

\subsection{Pattern Recognition}

\subsubsection{Perceptual Grouping}

In \cite{Probabilistic-tensor-voting-robust-perceptual-grouping}, a tensor voting approach is used to perform perceptual grouping on noisy data. The approach extends the standard tensor voting using Bayesian probabilities. The Bayesian framework has added benefits of	reducing the learning variance of the perceptual grouping task. Another	contribution of this paper is the use of a 2nd order tensor and two	types of polarity vectors to handle both outliers and inlier noise. The approach also is able to process all types of geometric structures. For probabilistic tensor voting, the voting procedure uses random vector with a given standard deviation. It uses three layers of voting:	sparse ball vote, sparse stick vote, and revised stick vote, where the goal is to change the positions of each token based on a random vector.


\subsubsection{Watershed Segmentation}

The Watershed Transform is a well know algorithm for segmenting meaningful regions from an image. The main benefit of the watershed technique as opposed to method, such as edge-based detection, is that the watershed produces closed boundaries, which allows contours to be detected. An overview of the Watershed algorithm	is presented in \cite{Watershed-transform-definition-algorithms-parallelization-strategies}.

In \cite{Image_segmentation_using_texture_gradient_based_watershed_transform}, segmentation is achieved using the watershed transform from texture gradients. The texture gradients are retrieved using various wavelet transforms.

Similarly, in \cite{Watershed-based-textural-image-segmentation}, an approach to texture segmentation is proposed. When applied to textures,	the watershed algorithm tends to over-segment a texture into small homogeneous sub-texture regions. Therefore, a texture can be determined	to be set of texture segments. The texture sub-regions can be merged using a clustering algorithm. A wavelet transform is used to characterize the irregular shaped segment regions, which are used to cluster similar	groups of sub-textures.

Another approach used in the Watershed algorithm is based on markers. The desired regions to segment can be extracted	using some known feature which gets marked on the image. In \cite{Fingerprint-pore-extraction-based-marker-controlled-watershed-segmentation}, this concept is applied to a fingerprint pore extraction application. Marker based segmentation has been shown to be robust for segmentations problems where the objects are closed contours with boundaries being ridges. In this application, the watershed transform is applied to the gradient image of the fingerprints, and the regional minimas are computed to obtain the markers, then compute the watershed transform based on the markers.

In \cite{Detection-breast-tumor-candidates-using-marker-controlled-watershed-segmentation}, the watershed segmentation is applied to a breast tumor detection application.


\subsection{Automated Feature Detection}

In \cite{Extraction_roads_multispectral_imagery}, a semi-automated method for extracting roads from aerial or satellite images is proposed. The purpose of this paper is to localize roads from higher resolution images, using some form of segmentation. The proposed method is semi-automated because a user must provide and initial seed point, onto which a region growing algorithm is applied to extract the road.

A level set method is applied to evolve the boundary of the road region, which can better handle noise and multiple road mergers. The boundary for the desired region is based on a speed function, which is designed based on uniformity, texture, and contrast properties. Once the regions have been extracted, the centerlines of the roads are estimated using a skeletonization procedure, with junction analysis to ensure that intersections are properly handled.

In \cite{Extracting_ocean_surface_feature_modis}, linear features of internal waves are extracted using a technique called \emph{Multiscale Retinex}(MSR) feature extraction. Although the problem set in \cite{Extracting_ocean_surface_feature_modis} is different, there appears to be significant overlap and comparison between dune crest-line detection and oceanic internal wave detection. The oceanic internal waves are typically generated from many sources such as tidal currents, ocean frontal boundaries, and other atmospheric conditions. The MSR is an image processing technique that provides consistency and dynamic range compression across an image with poor contrast. The paper discusses the use both the Wavelet Transform Modulus Maxima (WTMM) and the Canny edge detector, which turned out to have superior performance.

There has been some work done in automated dune detection. In \cite{BandeiraMarques}, a supervised learning approach is used, training classifiers such as Support Vector Machine and Random Forests to detect dune structures on Mars. The method proposed in this paper is to classify small (40 by 40 pixels) cells in a quantized image grid. In each cells features are computed based on the image gradients, using both phase and magnitude. In order to classify a cell as either a dune or not a dune, the features of both the cell and the cell's neighboring cells are used. The features extracted are then used to train the machine learning method, which is used to then predict if a cell is a dune.

Although this type of method has typically shown very good results, there are a few drawbacks with using supervised learning approaches. These types of methods usually require a fairly large labeled dataset which may not always be available. Anytime a dataset is constructed for this purpose, it is important to provide a large number of examples of different types of dunes, in order to get a robust representation of the problem set. In \cite{BandeiraMarques}, 230 labeled images were used to train and test the method, and have a decent representation of various dune types. Another drawback of this approach is the use of cells, for which fixed-sized cells may not be scale invariant. Also, quantizing an image into larger cells	will affect localization accuracy of the dunes. If the application requires higher localization accuracy, this type of supervised learning approach may not be suitable.


\subsection{Automated Dune Detection}

In \cite{Automated-mapping-linear-dunefield-morphometric-parameters}, an automated extraction of dune features is proposed. In this approach, the authors claim that the Canny edge detector provided poor results, and the sobel operator generated more consistent results. The gradient magnitude of the Sobel operator is used to threshold out the weaker edges. The use of a histogram to compute the orientations of the gradients was found to have a bimodal distribution, with one of the modes being the dune edges. Dune candidates are determined based on their gradient magnitude which filter out weaker candidates unless they are near strong candidates.